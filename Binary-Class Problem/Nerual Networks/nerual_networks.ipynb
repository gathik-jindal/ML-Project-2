{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14168734",
   "metadata": {},
   "source": [
    "# Neural Network for Binary Risk Classification\n",
    "\n",
    "## 1. Data Preprocessing & Embedding Preparation\n",
    "\n",
    "Neural Networks learn best when numerical features are scaled (normalized) and categorical features are fed into **Embedding Layers**.\n",
    "\n",
    "**Our Pipeline:**\n",
    "1.  **Numerical Features:** We use `StandardScaler` to force inputs (like `AnnualEarnings`) into a small range (mean 0, std 1). This prevents exploding gradients.\n",
    "2.  **Categorical Features:** We use **Label Encoding** (converting strings to integers `0, 1, 2...`). \n",
    "    * *Why?* We will feed these integers into an Embedding Layer in the next step, allowing the model to learn dense vector representations for each category.\n",
    "3.  **Target:** We isolate `RiskFlag`. Since this is binary classification, our target is a single vector of 0s and 1s.\n",
    "4.  **Validation Split:** We set aside 20% of the training data *now* so we can use it to monitor the neural network's learning curve later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2e928d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (163421, 16)\n",
      "Val Shape:   (40856, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ApplicantYears</th>\n",
       "      <th>AnnualEarnings</th>\n",
       "      <th>RequestedSum</th>\n",
       "      <th>TrustMetric</th>\n",
       "      <th>WorkDuration</th>\n",
       "      <th>ActiveAccounts</th>\n",
       "      <th>OfferRate</th>\n",
       "      <th>RepayPeriod</th>\n",
       "      <th>DebtFactor</th>\n",
       "      <th>QualificationLevel</th>\n",
       "      <th>WorkCategory</th>\n",
       "      <th>RelationshipStatus</th>\n",
       "      <th>OwnsProperty</th>\n",
       "      <th>FamilyObligation</th>\n",
       "      <th>FundUseCase</th>\n",
       "      <th>JointApplicant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107038</th>\n",
       "      <td>-0.766204</td>\n",
       "      <td>-0.314521</td>\n",
       "      <td>1.451876</td>\n",
       "      <td>1.579366</td>\n",
       "      <td>-0.187860</td>\n",
       "      <td>-1.344869</td>\n",
       "      <td>1.543367</td>\n",
       "      <td>1.415720</td>\n",
       "      <td>-1.388309</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97600</th>\n",
       "      <td>1.434507</td>\n",
       "      <td>-0.657328</td>\n",
       "      <td>-0.570017</td>\n",
       "      <td>-0.126359</td>\n",
       "      <td>-0.043541</td>\n",
       "      <td>-0.449530</td>\n",
       "      <td>0.836621</td>\n",
       "      <td>-1.417010</td>\n",
       "      <td>-0.435570</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6929</th>\n",
       "      <td>1.701260</td>\n",
       "      <td>1.088000</td>\n",
       "      <td>-1.014136</td>\n",
       "      <td>-0.101182</td>\n",
       "      <td>-0.938318</td>\n",
       "      <td>-0.449530</td>\n",
       "      <td>-1.628703</td>\n",
       "      <td>0.707538</td>\n",
       "      <td>-1.345003</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89894</th>\n",
       "      <td>1.167755</td>\n",
       "      <td>-0.963679</td>\n",
       "      <td>-1.408421</td>\n",
       "      <td>1.654896</td>\n",
       "      <td>-1.457866</td>\n",
       "      <td>0.445809</td>\n",
       "      <td>-1.328826</td>\n",
       "      <td>-0.708827</td>\n",
       "      <td>-0.045813</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69819</th>\n",
       "      <td>-1.166333</td>\n",
       "      <td>-0.510275</td>\n",
       "      <td>1.688105</td>\n",
       "      <td>-1.284490</td>\n",
       "      <td>1.255329</td>\n",
       "      <td>-0.449530</td>\n",
       "      <td>1.516243</td>\n",
       "      <td>0.707538</td>\n",
       "      <td>-1.561534</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ApplicantYears  AnnualEarnings  RequestedSum  TrustMetric  \\\n",
       "107038       -0.766204       -0.314521      1.451876     1.579366   \n",
       "97600         1.434507       -0.657328     -0.570017    -0.126359   \n",
       "6929          1.701260        1.088000     -1.014136    -0.101182   \n",
       "89894         1.167755       -0.963679     -1.408421     1.654896   \n",
       "69819        -1.166333       -0.510275      1.688105    -1.284490   \n",
       "\n",
       "        WorkDuration  ActiveAccounts  OfferRate  RepayPeriod  DebtFactor  \\\n",
       "107038     -0.187860       -1.344869   1.543367     1.415720   -1.388309   \n",
       "97600      -0.043541       -0.449530   0.836621    -1.417010   -0.435570   \n",
       "6929       -0.938318       -0.449530  -1.628703     0.707538   -1.345003   \n",
       "89894      -1.457866        0.445809  -1.328826    -0.708827   -0.045813   \n",
       "69819       1.255329       -0.449530   1.516243     0.707538   -1.561534   \n",
       "\n",
       "        QualificationLevel  WorkCategory  RelationshipStatus  OwnsProperty  \\\n",
       "107038                   3             1                   2             0   \n",
       "97600                    1             2                   2             0   \n",
       "6929                     3             1                   2             0   \n",
       "89894                    0             0                   0             0   \n",
       "69819                    0             2                   2             2   \n",
       "\n",
       "        FamilyObligation  FundUseCase  JointApplicant  \n",
       "107038                 0            2               0  \n",
       "97600                  0            2               0  \n",
       "6929                   0            0               0  \n",
       "89894                  2            1               2  \n",
       "69819                  2            1               2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Configuration ---\n",
    "# 1. Categorical Features (To be Label Encoded for Embeddings)\n",
    "CAT_COLS = ['QualificationLevel', 'WorkCategory', 'RelationshipStatus', \n",
    "            'OwnsProperty', 'FamilyObligation', 'FundUseCase', 'JointApplicant']\n",
    "\n",
    "# 2. Numerical Features (To be Scaled)\n",
    "NUM_COLS = ['ApplicantYears', 'AnnualEarnings', 'RequestedSum', 'TrustMetric', \n",
    "            'WorkDuration', 'ActiveAccounts', 'OfferRate', 'RepayPeriod', 'DebtFactor']\n",
    "\n",
    "def preprocess_for_nn(df, is_train=True, encoders=None):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Separation\n",
    "    target = None\n",
    "    if 'RiskFlag' in df.columns:\n",
    "        target = df['RiskFlag'].values\n",
    "        df = df.drop(columns=['RiskFlag'])\n",
    "    \n",
    "    if 'ProfileID' in df.columns:\n",
    "        df = df.drop(columns=['ProfileID'])\n",
    "        \n",
    "    # Initialize storage for encoders if training\n",
    "    if is_train:\n",
    "        encoders = {\n",
    "            'scaler': StandardScaler(),\n",
    "            'label_encoders': {}\n",
    "        }\n",
    "    \n",
    "    # 2. Scale Numerical Columns\n",
    "    if is_train:\n",
    "        df[NUM_COLS] = encoders['scaler'].fit_transform(df[NUM_COLS])\n",
    "    else:\n",
    "        df[NUM_COLS] = encoders['scaler'].transform(df[NUM_COLS])\n",
    "        \n",
    "    # 3. Label Encode Categorical Columns\n",
    "    for col in CAT_COLS:\n",
    "        df[col] = df[col].astype(str) # Handle potential mixed types\n",
    "        \n",
    "        if is_train:\n",
    "            le = LabelEncoder()\n",
    "            # Fit on unique values + 'Unknown' for safety\n",
    "            unique_vals = list(df[col].unique()) + ['Unknown']\n",
    "            le.fit(unique_vals)\n",
    "            encoders['label_encoders'][col] = le\n",
    "            df[col] = le.transform(df[col])\n",
    "        else:\n",
    "            le = encoders['label_encoders'][col]\n",
    "            # Map unseen labels to 'Unknown'\n",
    "            df[col] = df[col].map(lambda s: s if s in le.classes_ else 'Unknown')\n",
    "            df[col] = le.transform(df[col])\n",
    "            \n",
    "    return df, target, encoders\n",
    "\n",
    "# --- Load Data (Placeholder) ---\n",
    "# Load your data here\n",
    "dataset_dir = Path().cwd().parent / \"datasets\" / \"preprocessed\"\n",
    "train = pd.read_csv(dataset_dir / \"train_preprocessed.csv\")\n",
    "test = pd.read_csv(dataset_dir / \"test_preprocessed.csv\")\n",
    "\n",
    "# --- Apply Preprocessing ---\n",
    "# 1. Process Train\n",
    "X_processed, y_processed, encoders = preprocess_for_nn(train, is_train=True)\n",
    "\n",
    "# 2. Create Validation Split (Essential for NN Hyperparameter Tuning)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_processed, y_processed, test_size=0.2, random_state=42, stratify=y_processed\n",
    ")\n",
    "\n",
    "print(f\"Train Shape: {X_train.shape}\")\n",
    "print(f\"Val Shape:   {X_val.shape}\")\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652398bd",
   "metadata": {},
   "source": [
    "## 2. Neural Network Architecture & Hyperparameter Tuning\n",
    "\n",
    "We use the Keras **Functional API** to build a custom architecture:\n",
    "1.  **Embedding Branch:** Each categorical feature (e.g., `WorkCategory`) gets its own Input and Embedding Layer. This allows the network to learn a dense vector for \"Self-Employed\" vs \"Salaried\", capturing semantic relationships.\n",
    "2.  **Numerical Branch:** Numerical features are fed directly into a Dense layer.\n",
    "3.  **Concatenation:** The outputs of both branches are merged and passed through deep Dense layers.\n",
    "\n",
    "**Hyperparameter Search:**\n",
    "Instead of guessing, we loop through combinations of:\n",
    "* **Units:** Capacity of the network (e.g., 64 vs 128 neurons).\n",
    "* **Dropout:** Regularization strength (to prevent overfitting).\n",
    "* **Learning Rate:** Step size for the optimizer.\n",
    "\n",
    "We use **EarlyStopping** to automatically halt training if the validation loss stops improving, saving time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d4ab53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Neural Network Grid Search...\n",
      "\n",
      "Testing: {'units': 64, 'dropout': 0.3, 'lr': 0.001}\n",
      "  -> Best Val Loss: 0.3124\n",
      "\n",
      "Testing: {'units': 128, 'dropout': 0.3, 'lr': 0.001}\n",
      "  -> Best Val Loss: 0.3125\n",
      "\n",
      "Testing: {'units': 128, 'dropout': 0.5, 'lr': 0.0005}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "\n",
    "# --- 1. Data Preparation for Functional API ---\n",
    "# Keras requires a list of inputs: [Cat1_Array, Cat2_Array, ..., CatN_Array, Numerical_Matrix]\n",
    "def get_keras_inputs(df, cat_cols, num_cols):\n",
    "    inputs = []\n",
    "    # 1. Categorical Inputs (One array per column)\n",
    "    for col in cat_cols:\n",
    "        inputs.append(df[col].values)\n",
    "    \n",
    "    # 2. Numerical Inputs (One matrix for all)\n",
    "    inputs.append(df[num_cols].values)\n",
    "    return inputs\n",
    "\n",
    "# Format Training and Validation data\n",
    "# (Assuming X_train, X_val, y_train, y_val exist from Step 1's train_test_split)\n",
    "X_train_inputs = get_keras_inputs(X_train, CAT_COLS, NUM_COLS)\n",
    "X_val_inputs = get_keras_inputs(X_val, CAT_COLS, NUM_COLS)\n",
    "\n",
    "# --- 2. Model Builder Function ---\n",
    "def build_model(hp_units, hp_dropout, hp_lr):\n",
    "    inputs_list = []\n",
    "    embeddings = []\n",
    "    \n",
    "    # A. Embedding Branch\n",
    "    for col in CAT_COLS:\n",
    "        # Input for single integer\n",
    "        input_cat = layers.Input(shape=(1,), name=f\"in_{col}\")\n",
    "        inputs_list.append(input_cat)\n",
    "        \n",
    "        # Embedding Dimension Rule of Thumb: min(50, unique/2)\n",
    "        n_unique = X_train[col].nunique()\n",
    "        embed_dim = min(50, (n_unique // 2) + 1)\n",
    "        \n",
    "        # Layer\n",
    "        emb = layers.Embedding(input_dim=n_unique + 1, output_dim=embed_dim, name=f\"emb_{col}\")(input_cat)\n",
    "        emb = layers.Flatten()(emb)\n",
    "        embeddings.append(emb)\n",
    "        \n",
    "    # B. Numerical Branch\n",
    "    input_num = layers.Input(shape=(len(NUM_COLS),), name=\"in_numerics\")\n",
    "    inputs_list.append(input_num)\n",
    "    \n",
    "    # C. Merge\n",
    "    x = layers.Concatenate()(embeddings + [input_num])\n",
    "    \n",
    "    # D. Dense Layers\n",
    "    x = layers.Dense(hp_units, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(hp_dropout)(x)\n",
    "    \n",
    "    x = layers.Dense(hp_units // 2, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(hp_dropout)(x)\n",
    "    \n",
    "    # Output (Binary Classification -> Sigmoid)\n",
    "    output = layers.Dense(1, activation='sigmoid', name=\"output\")(x)\n",
    "    \n",
    "    # Compile\n",
    "    model = models.Model(inputs=inputs_list, outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=hp_lr),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# --- 3. Hyperparameter Tuning Loop ---\n",
    "param_grid = [\n",
    "    {'units': 64, 'dropout': 0.3, 'lr': 0.001},\n",
    "    {'units': 128, 'dropout': 0.3, 'lr': 0.001},\n",
    "    {'units': 128, 'dropout': 0.5, 'lr': 0.0005}, # High regularization\n",
    "]\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "best_history = None\n",
    "best_params = {}\n",
    "\n",
    "print(\"Starting Neural Network Grid Search...\")\n",
    "\n",
    "for params in param_grid:\n",
    "    print(f\"\\nTesting: {params}\")\n",
    "    \n",
    "    model = build_model(params['units'], params['dropout'], params['lr'])\n",
    "    \n",
    "    es = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_inputs, y_train,\n",
    "        validation_data=(X_val_inputs, y_val),\n",
    "        epochs=30,\n",
    "        batch_size=64,\n",
    "        callbacks=[es],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    print(f\"  -> Best Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "        best_history = history\n",
    "        best_params = params\n",
    "\n",
    "print(f\"\\nWINNER: {best_params} with Val Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# --- 4. Plot Training History ---\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    ax1.plot(epochs, acc, 'bo-', label='Training Acc')\n",
    "    ax1.plot(epochs, val_acc, 'ro-', label='Validation Acc')\n",
    "    ax1.set_title('Accuracy')\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.plot(epochs, loss, 'b--', label='Training Loss')\n",
    "    ax2.plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
    "    ax2.set_title('Loss')\n",
    "    ax2.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(best_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c969a8bb",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation\n",
    "\n",
    "We evaluate the `best_model` on the **Validation Set**. This tells us how well the model generalizes to new data.\n",
    "\n",
    "**Key Metrics:**\n",
    "1.  **Confusion Matrix:** Shows us exactly where the model makes mistakes.\n",
    "    * *False Negatives (FN):* High Risk applicants we missed (Dangerous!).\n",
    "    * *False Positives (FP):* Safe applicants we flagged as risky (Annoying).\n",
    "2.  **ROC Curve & AUC:** Measures the model's ability to distinguish between classes. An AUC of 0.5 is random guessing; 1.0 is perfect.\n",
    "3.  **F1-Score:** The harmonic mean of precision and recall, useful if the classes are imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95863402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "\n",
    "# --- 1. Make Predictions on Validation Set ---\n",
    "# The model outputs a probability between 0 and 1\n",
    "y_pred_prob = best_model.predict(X_val_inputs)\n",
    "\n",
    "# Convert probability to class (0 or 1) using a threshold of 0.5\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# --- 2. Classification Report ---\n",
    "print(\"\\n=== Model Performance (Validation Set) ===\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# --- 3. Confusion Matrix Visualization ---\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Predicted Low Risk', 'Predicted High Risk'],\n",
    "            yticklabels=['Actual Low Risk', 'Actual High Risk'])\n",
    "plt.title('Neural Network Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# --- 4. ROC Curve ---\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--') # Random guess line\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# --- 5. Precision-Recall Curve (Optional but good for imbalanced data) ---\n",
    "precision, recall, _ = precision_recall_curve(y_val, y_pred_prob)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='green', lw=2)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b54d039",
   "metadata": {},
   "source": [
    "## 4. Retraining on Full Data & Submission\n",
    "\n",
    "We found the best architecture using the Validation Set. Now, we deploy it.\n",
    "\n",
    "**The Workflow:**\n",
    "1.  **Retrain:** We build a fresh model using the winning parameters (`best_params`) and train it on the **full training set** (no validation split). This gives the model maximum data to learn from.\n",
    "2.  **Process Test Data:** We apply the *exact same* Label Encoders and Scalers used on the training data to the Test data.\n",
    "3.  **Predict:** We generate probabilities for the test set.\n",
    "4.  **Thresholding:** Since this is binary classification, the model outputs a number between 0 and 1. We apply a threshold (0.5) to convert this into a hard `RiskFlag` (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30906c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Prepare Full Training Data ---\n",
    "# We use the full X_processed and y_processed from Step 1 (before the train/val split)\n",
    "# If variables were lost, ensure you run Step 1's preprocessing on the full 'train' df again.\n",
    "full_train_inputs = get_keras_inputs(X_processed, CAT_COLS, NUM_COLS)\n",
    "full_train_labels = y_processed\n",
    "\n",
    "print(f\"Retraining on full dataset: {len(full_train_labels)} samples\")\n",
    "print(f\"Best Params: {best_params}\")\n",
    "\n",
    "# --- 2. Build & Train Final Model ---\n",
    "# Re-instantiate the model structure with the winner params\n",
    "final_model = build_model(\n",
    "    hp_units=best_params['units'],\n",
    "    hp_dropout=best_params['dropout'],\n",
    "    hp_lr=best_params['lr']\n",
    ")\n",
    "\n",
    "# Fit on ALL data\n",
    "# We train for a fixed number of epochs (e.g., 20-30) since we can't use EarlyStopping without a val set.\n",
    "# Ideally, you use the epoch number where val_loss stopped improving in Step 2.\n",
    "final_model.fit(\n",
    "    full_train_inputs, \n",
    "    full_train_labels,\n",
    "    epochs=25, \n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- 3. Process Test Data ---\n",
    "# Apply the encoders fitted on Train to Test\n",
    "# Note: 'test' df should be loaded from the CSV\n",
    "X_test_processed, _, _ = preprocess_for_nn(test, is_train=False, encoders=encoders)\n",
    "\n",
    "# Format for Keras\n",
    "test_inputs = get_keras_inputs(X_test_processed, CAT_COLS, NUM_COLS)\n",
    "\n",
    "# --- 4. Generate Predictions ---\n",
    "print(\"\\nPredicting on Test Set...\")\n",
    "test_probs = final_model.predict(test_inputs)\n",
    "\n",
    "# Thresholding (0.5 is standard, but you can tune this based on Precision-Recall needs)\n",
    "test_preds = (test_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# --- 5. Create Submission CSV ---\n",
    "submission = pd.DataFrame({\n",
    "    'ProfileID': test['ProfileID'],\n",
    "    'RiskFlag': test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_supervised_neural_network.csv\", index=False)\n",
    "print(\"Saved: submission_supervised_neural_network.csv\")\n",
    "\n",
    "# Quick check of class balance in prediction\n",
    "print(\"\\nPredicted Class Distribution:\")\n",
    "print(submission['RiskFlag'].value_counts(normalize=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
