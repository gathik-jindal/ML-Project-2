{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "821cd9fa",
   "metadata": {},
   "source": [
    "# Preprocessing for the Travel dataset\n",
    "\n",
    "This notebook outlines the steps taken to preprocess the Travel dataset for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f40fb5",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d920595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd09ea91",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfa17c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dataset: train.csv , test.csv\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = Path.cwd().joinpath(\"datasets\")\n",
    "\n",
    "train_dataset = \"train.csv\"\n",
    "test_dataset = \"test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(dataset_dir.joinpath(train_dataset))\n",
    "test_df = pd.read_csv(dataset_dir.joinpath(test_dataset))\n",
    "\n",
    "print(\"Successfully loaded dataset:\", train_dataset, \",\", test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e31cbba",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d05e09f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EXPLORATORY DATA ANALYSIS for train\n",
      "==================================================\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 204277 entries, 0 to 204276\n",
      "Data columns (total 18 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   ProfileID           204277 non-null  object \n",
      " 1   ApplicantYears      204277 non-null  int64  \n",
      " 2   AnnualEarnings      204277 non-null  int64  \n",
      " 3   RequestedSum        204277 non-null  int64  \n",
      " 4   TrustMetric         204277 non-null  int64  \n",
      " 5   WorkDuration        204277 non-null  int64  \n",
      " 6   ActiveAccounts      204277 non-null  int64  \n",
      " 7   OfferRate           204277 non-null  float64\n",
      " 8   RepayPeriod         204277 non-null  int64  \n",
      " 9   DebtFactor          204277 non-null  float64\n",
      " 10  QualificationLevel  204277 non-null  object \n",
      " 11  WorkCategory        204277 non-null  object \n",
      " 12  RelationshipStatus  204277 non-null  object \n",
      " 13  OwnsProperty        204277 non-null  object \n",
      " 14  FamilyObligation    204277 non-null  object \n",
      " 15  FundUseCase         204277 non-null  object \n",
      " 16  JointApplicant      204277 non-null  object \n",
      " 17  RiskFlag            204277 non-null  int64  \n",
      "dtypes: float64(2), int64(8), object(8)\n",
      "memory usage: 28.1+ MB\n",
      "None\n",
      "\n",
      "Dataset Shape: (204277, 18)\n",
      "\n",
      "Statistical Summary:\n",
      "       ApplicantYears  AnnualEarnings   RequestedSum    TrustMetric  \\\n",
      "count   204277.000000   204277.000000  204277.000000  204277.000000   \n",
      "mean        43.489340    82506.227980  127547.496395     574.075500   \n",
      "std         14.995191    38952.103374   70855.064746     158.877098   \n",
      "min         18.000000    15000.000000    5001.000000     300.000000   \n",
      "25%         31.000000    48878.000000   66059.000000     437.000000   \n",
      "50%         43.000000    82400.000000  127603.000000     574.000000   \n",
      "75%         56.000000   116247.000000  188843.000000     712.000000   \n",
      "max         69.000000   149999.000000  249999.000000     849.000000   \n",
      "\n",
      "        WorkDuration  ActiveAccounts      OfferRate    RepayPeriod  \\\n",
      "count  204277.000000   204277.000000  204277.000000  204277.000000   \n",
      "mean       59.508511        2.502078      13.488147      36.010926   \n",
      "std        34.645589        1.116898       6.636060      16.944827   \n",
      "min         0.000000        1.000000       2.000000      12.000000   \n",
      "25%        30.000000        2.000000       7.760000      24.000000   \n",
      "50%        59.000000        3.000000      13.450000      36.000000   \n",
      "75%        90.000000        4.000000      19.240000      48.000000   \n",
      "max       119.000000        4.000000      25.000000      60.000000   \n",
      "\n",
      "          DebtFactor       RiskFlag  \n",
      "count  204277.000000  204277.000000  \n",
      "mean        0.500579       0.116278  \n",
      "std         0.230914       0.320559  \n",
      "min         0.100000       0.000000  \n",
      "25%         0.300000       0.000000  \n",
      "50%         0.500000       0.000000  \n",
      "75%         0.700000       0.000000  \n",
      "max         0.900000       1.000000  \n",
      "\n",
      "Missing Values:\n",
      "Empty DataFrame\n",
      "Columns: [Missing_Count, Percentage]\n",
      "Index: []\n",
      "\n",
      "Data Types:\n",
      "object     8\n",
      "int64      8\n",
      "float64    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Duplicate Rows: 0\n",
      "\n",
      "Unique Values in Categorical Columns:\n",
      "ProfileID: 204277 unique values\n",
      "QualificationLevel: 4 unique values\n",
      "   -> [Low Cardinality] Values: ['High School', \"Master's\", \"Bachelor's\", 'PhD']\n",
      "WorkCategory: 4 unique values\n",
      "   -> [Low Cardinality] Values: ['Self-employed', 'Unemployed', 'Part-time', 'Full-time']\n",
      "RelationshipStatus: 3 unique values\n",
      "   -> [Low Cardinality] Values: ['Single', 'Divorced', 'Married']\n",
      "OwnsProperty: 2 unique values\n",
      "   -> [Low Cardinality] Values: ['Yes', 'No']\n",
      "FamilyObligation: 2 unique values\n",
      "   -> [Low Cardinality] Values: ['No', 'Yes']\n",
      "FundUseCase: 5 unique values\n",
      "   -> [Low Cardinality] Values: ['Business', 'Education', 'Other', 'Auto', 'Home']\n",
      "JointApplicant: 2 unique values\n",
      "   -> [Low Cardinality] Values: ['No', 'Yes']\n",
      "\n",
      "==================================================\n",
      "EXPLORATORY DATA ANALYSIS for test\n",
      "==================================================\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51070 entries, 0 to 51069\n",
      "Data columns (total 17 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   ProfileID           51070 non-null  object \n",
      " 1   ApplicantYears      51070 non-null  int64  \n",
      " 2   AnnualEarnings      51070 non-null  int64  \n",
      " 3   RequestedSum        51070 non-null  int64  \n",
      " 4   TrustMetric         51070 non-null  int64  \n",
      " 5   WorkDuration        51070 non-null  int64  \n",
      " 6   ActiveAccounts      51070 non-null  int64  \n",
      " 7   OfferRate           51070 non-null  float64\n",
      " 8   RepayPeriod         51070 non-null  int64  \n",
      " 9   DebtFactor          51070 non-null  float64\n",
      " 10  QualificationLevel  51070 non-null  object \n",
      " 11  WorkCategory        51070 non-null  object \n",
      " 12  RelationshipStatus  51070 non-null  object \n",
      " 13  OwnsProperty        51070 non-null  object \n",
      " 14  FamilyObligation    51070 non-null  object \n",
      " 15  FundUseCase         51070 non-null  object \n",
      " 16  JointApplicant      51070 non-null  object \n",
      "dtypes: float64(2), int64(7), object(8)\n",
      "memory usage: 6.6+ MB\n",
      "None\n",
      "\n",
      "Dataset Shape: (51070, 17)\n",
      "\n",
      "Statistical Summary:\n",
      "       ApplicantYears  AnnualEarnings   RequestedSum   TrustMetric  \\\n",
      "count    51070.000000    51070.000000   51070.000000  51070.000000   \n",
      "mean        43.534169    82471.611474  127704.340141    575.019718   \n",
      "std         14.970605    39006.993391   70783.797718    159.010208   \n",
      "min         18.000000    15000.000000    5000.000000    300.000000   \n",
      "25%         31.000000    48616.750000   66506.250000    437.000000   \n",
      "50%         43.000000    82686.500000  127330.000000    575.000000   \n",
      "75%         57.000000   116136.250000  189465.750000    713.000000   \n",
      "max         69.000000   149994.000000  249986.000000    849.000000   \n",
      "\n",
      "       WorkDuration  ActiveAccounts     OfferRate   RepayPeriod    DebtFactor  \n",
      "count  51070.000000    51070.000000  51070.000000  51070.000000  51070.000000  \n",
      "mean      59.675837        2.496867     13.511278     36.085765      0.498745  \n",
      "std       34.634536        1.117497      6.638008     17.067022      0.230924  \n",
      "min        0.000000        1.000000      2.000000     12.000000      0.100000  \n",
      "25%       30.000000        1.000000      7.800000     24.000000      0.300000  \n",
      "50%       60.000000        2.000000     13.480000     36.000000      0.500000  \n",
      "75%       90.000000        3.000000     19.280000     48.000000      0.700000  \n",
      "max      119.000000        4.000000     25.000000     60.000000      0.900000  \n",
      "\n",
      "Missing Values:\n",
      "Empty DataFrame\n",
      "Columns: [Missing_Count, Percentage]\n",
      "Index: []\n",
      "\n",
      "Data Types:\n",
      "object     8\n",
      "int64      7\n",
      "float64    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Duplicate Rows: 0\n",
      "\n",
      "Unique Values in Categorical Columns:\n",
      "ProfileID: 51070 unique values\n",
      "QualificationLevel: 4 unique values\n",
      "   -> [Low Cardinality] Values: ['PhD', 'High School', \"Master's\", \"Bachelor's\"]\n",
      "WorkCategory: 4 unique values\n",
      "   -> [Low Cardinality] Values: ['Self-employed', 'Part-time', 'Unemployed', 'Full-time']\n",
      "RelationshipStatus: 3 unique values\n",
      "   -> [Low Cardinality] Values: ['Single', 'Divorced', 'Married']\n",
      "OwnsProperty: 2 unique values\n",
      "   -> [Low Cardinality] Values: ['Yes', 'No']\n",
      "FamilyObligation: 2 unique values\n",
      "   -> [Low Cardinality] Values: ['Yes', 'No']\n",
      "FundUseCase: 5 unique values\n",
      "   -> [Low Cardinality] Values: ['Home', 'Education', 'Business', 'Other', 'Auto']\n",
      "JointApplicant: 2 unique values\n",
      "   -> [Low Cardinality] Values: ['No', 'Yes']\n"
     ]
    }
   ],
   "source": [
    "def perform_eda(df, save=True, config_file=\"categorical_config.json\"):\n",
    "    \"\"\"\n",
    "    Perform basic exploratory data analysis.\n",
    "    Saves low-cardinality categorical values to a JSON config file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dataset Info\n",
    "    print(\"\\nDataset Info:\")\n",
    "    print(df.info())\n",
    "\n",
    "    # Shape\n",
    "    print(f\"\\nDataset Shape: {df.shape}\")\n",
    "\n",
    "    # Statistical summary\n",
    "    print(\"\\nStatistical Summary:\")\n",
    "    print(df.describe())\n",
    "\n",
    "    # Missing values\n",
    "    print(\"\\nMissing Values:\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing_Count': missing,\n",
    "        'Percentage': missing_pct\n",
    "    })\n",
    "    print(missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False))\n",
    "\n",
    "    # Data types\n",
    "    print(\"\\nData Types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "\n",
    "    # Duplicate rows\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nDuplicate Rows: {duplicates}\")\n",
    "\n",
    "    # --- MODIFIED SECTION START ---\n",
    "    print(\"\\nUnique Values in Categorical Columns:\")\n",
    "    \n",
    "    # Included 'category' dtype as well as object\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    \n",
    "    # Dictionary to store values for the config file\n",
    "    low_cardinality_map = {}\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        unique_count = df[col].nunique()\n",
    "        print(f\"{col}: {unique_count} unique values\")\n",
    "        \n",
    "        # Logic: If less than 20 unique values\n",
    "        if unique_count < 20:\n",
    "            # Get unique values and convert to a standard Python list\n",
    "            # We convert to str to ensure it is JSON serializable (handles np.nan objects)\n",
    "            unique_vals = [str(x) for x in df[col].unique().tolist()]\n",
    "            \n",
    "            # 1. Print them\n",
    "            print(f\"   -> [Low Cardinality] Values: {unique_vals}\")\n",
    "            \n",
    "            # 2. Add to dictionary for saving\n",
    "            low_cardinality_map[col] = unique_vals\n",
    "\n",
    "    # Save to config file if we found any matching columns\n",
    "    if low_cardinality_map and save:\n",
    "        try:\n",
    "            with open(config_file, 'w') as f:\n",
    "                json.dump(low_cardinality_map, f, indent=4)\n",
    "            print(f\"\\n[SUCCESS] Low cardinality categories saved to '{config_file}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n[ERROR] Could not save config file: {e}\")\n",
    "    # --- MODIFIED SECTION END ---\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPLORATORY DATA ANALYSIS for train\")\n",
    "print(\"=\"*50)\n",
    "d = perform_eda(train_df, False)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPLORATORY DATA ANALYSIS for test\")\n",
    "print(\"=\"*50)\n",
    "d = perform_eda(test_df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2bbd9d",
   "metadata": {},
   "source": [
    "## Understanding the Dataset\n",
    "\n",
    "Train and Test datasets each have a ton of missing values, most of them are less than 5% missing, but some columns have more than 50% missing values.\n",
    "\n",
    "Two options, drop the values of those columns or impute them.\n",
    "\n",
    "For now, we will drop datapoints for which columns have less than 10% missing values and for columns with more than 10% missing values those columns will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae3a6f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "HANDLING MISSING VALUES (Corrected)\n",
      "==================================================\n",
      "Processing TRAIN data...\n",
      "Dropped rows with missing values. New shape: (204277, 18)\n",
      "\n",
      "Missing values after imputation: 0\n",
      "\n",
      "Missing values after imputation: 0\n",
      "\n",
      "Processing TEST data...\n",
      "Imputed 9 numerical columns with median\n",
      "Imputed 8 categorical columns with mode\n",
      "\n",
      "Missing values after imputation: 0\n",
      "\n",
      "Missing values after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "def handle_missing_values(df, strategy='auto'):\n",
    "    \"\"\"\n",
    "    Handle missing values using different strategies\n",
    "\n",
    "    Parameters:\n",
    "    - strategy: 'auto', 'mean', 'median', 'mode', 'drop', 'drop_column, or custom dict\n",
    "    \"\"\"\n",
    "\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Identify numerical and categorical columns\n",
    "    numerical_cols = df_copy.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_cols = df_copy.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    if strategy == 'auto':\n",
    "        # For numerical: use median\n",
    "        if numerical_cols:\n",
    "            num_imputer = SimpleImputer(strategy='median')\n",
    "            df_copy[numerical_cols] = num_imputer.fit_transform(df_copy[numerical_cols])\n",
    "            print(f\"Imputed {len(numerical_cols)} numerical columns with median\")\n",
    "\n",
    "        # For categorical: use mode\n",
    "        if categorical_cols:\n",
    "            for col in categorical_cols:\n",
    "                if df_copy[col].isnull().sum() > 0:\n",
    "                    mode_value = df_copy[col].mode()[0] if len(df_copy[col].mode()) > 0 else 'Unknown'\n",
    "                    df_copy[col].fillna(mode_value, inplace=True)\n",
    "            print(f\"Imputed {len(categorical_cols)} categorical columns with mode\")\n",
    "\n",
    "    elif strategy == 'mean':\n",
    "        num_imputer = SimpleImputer(strategy='mean')\n",
    "        df_copy[numerical_cols] = num_imputer.fit_transform(df_copy[numerical_cols])\n",
    "\n",
    "    elif strategy == 'median':\n",
    "        num_imputer = SimpleImputer(strategy='median')\n",
    "        df_copy[numerical_cols] = num_imputer.fit_transform(df_copy[numerical_cols])\n",
    "\n",
    "    elif strategy == 'drop':\n",
    "        # 1. Calculate the percentage of missing values for each column\n",
    "        null_percentages = df_copy.isnull().mean()\n",
    "\n",
    "        # 2. Identify columns where missing values are less than 10% (0.1)\n",
    "        # We also ensure > 0 to avoid checking columns that are already full\n",
    "        cols_to_check = null_percentages[\n",
    "            (null_percentages < 0.10) & (null_percentages > 0)\n",
    "        ].index\n",
    "\n",
    "        # 3. Drop rows only if they have NaN in those specific columns\n",
    "        df_copy = df_copy.dropna(subset=cols_to_check)\n",
    "        print(f\"Dropped rows with missing values. New shape: {df_copy.shape}\")\n",
    "    \n",
    "    elif strategy == 'drop_column':\n",
    "        # Drop columns with more than 10% missing values\n",
    "        df_copy = df_copy.drop(columns=df_copy.columns[df_copy.isnull().sum() > 0.1 * len(df_copy)])\n",
    "\n",
    "    print(f\"\\nMissing values after imputation: {df_copy.isnull().sum().sum()}\")\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"HANDLING MISSING VALUES (Corrected)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- TRAIN DATA ---\n",
    "# It is okay to drop rows in training if that's your preferred strategy\n",
    "print(\"Processing TRAIN data...\")\n",
    "train_df = handle_missing_values(train_df, strategy='drop')         # Drop rows with missing values\n",
    "train_df = handle_missing_values(train_df, strategy='drop_column')  # Drop columns with too many missing values\n",
    "\n",
    "# --- TEST DATA ---\n",
    "# NEVER drop rows in test data. Use imputation ('auto') instead.\n",
    "print(\"\\nProcessing TEST data...\")\n",
    "test_df = handle_missing_values(test_df, strategy='auto')          # <--- CHANGED: Impute instead of drop\n",
    "test_df = handle_missing_values(test_df, strategy='drop_column')   # Drop columns (risky* see note below)\n",
    "\n",
    "# *NOTE ON DROPPING COLUMNS: \n",
    "# If 'drop_column' removes a column in Train but keeps it in Test (or vice versa), \n",
    "# your model will crash due to mismatched features. \n",
    "# A safer way is to align them at the end:\n",
    "train_cols = train_df.columns.tolist()\n",
    "# Ensure test only has columns that are in train (excluding target 'RiskFlag')\n",
    "cols_to_keep = [c for c in train_cols if c != 'RiskFlag']\n",
    "test_df = test_df[cols_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb5eafe",
   "metadata": {},
   "source": [
    "## Detecting Outliers\n",
    "\n",
    "We're using the IQR method to detect outliers in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "152c7669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "OUTLIER DETECTION for train dataset\n",
      "==================================================\n",
      "ApplicantYears: 0 outliers detected\n",
      "AnnualEarnings: 0 outliers detected\n",
      "RequestedSum: 0 outliers detected\n",
      "TrustMetric: 0 outliers detected\n",
      "WorkDuration: 0 outliers detected\n",
      "ActiveAccounts: 0 outliers detected\n",
      "OfferRate: 0 outliers detected\n",
      "RepayPeriod: 0 outliers detected\n",
      "DebtFactor: 0 outliers detected\n",
      "RiskFlag: 23753 outliers detected\n",
      "\n",
      "==================================================\n",
      "OUTLIER DETECTION for test dataset\n",
      "==================================================\n",
      "ApplicantYears: 0 outliers detected\n",
      "AnnualEarnings: 0 outliers detected\n",
      "RequestedSum: 0 outliers detected\n",
      "TrustMetric: 0 outliers detected\n",
      "WorkDuration: 0 outliers detected\n",
      "ActiveAccounts: 0 outliers detected\n",
      "OfferRate: 0 outliers detected\n",
      "RepayPeriod: 0 outliers detected\n",
      "DebtFactor: 0 outliers detected\n"
     ]
    }
   ],
   "source": [
    "def detect_outliers_iqr(df, columns=None, threshold=1.5):\n",
    "\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "    outlier_indices = []\n",
    "\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower_bound = Q1 - threshold * IQR\n",
    "        upper_bound = Q3 + threshold * IQR\n",
    "\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index\n",
    "        outlier_indices.extend(outliers)\n",
    "\n",
    "        print(f\"{col}: {len(outliers)} outliers detected\")\n",
    "\n",
    "    return list(set(outlier_indices))\n",
    "\n",
    "\"\"\"Detect outliers using IQR method\"\"\"\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"OUTLIER DETECTION for train dataset\")\n",
    "print(\"=\"*50)\n",
    "# indices of outlers in the dataset\n",
    "outliers_train = detect_outliers_iqr(train_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"OUTLIER DETECTION for test dataset\")\n",
    "print(\"=\"*50)\n",
    "# indices of outlers in the dataset\n",
    "outliers_test = detect_outliers_iqr(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ae84c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessed datasets saved to c:\\Users\\gathi\\projects\\ML-Project-2\\Binary-Class Problem\\datasets\\preprocessed\n"
     ]
    }
   ],
   "source": [
    "save_dir = dataset_dir.joinpath(\"preprocessed\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "train_df.to_csv(save_dir.joinpath(\"train_preprocessed.csv\"), index=False)\n",
    "test_df.to_csv(save_dir.joinpath(\"test_preprocessed.csv\"), index=False)\n",
    "print(f\"\\nPreprocessed datasets saved to {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a99015e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EXPLORATORY DATA ANALYSIS for train\n",
      "==================================================\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 204277 entries, 0 to 204276\n",
      "Data columns (total 18 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   ProfileID           204277 non-null  object \n",
      " 1   ApplicantYears      204277 non-null  int64  \n",
      " 2   AnnualEarnings      204277 non-null  int64  \n",
      " 3   RequestedSum        204277 non-null  int64  \n",
      " 4   TrustMetric         204277 non-null  int64  \n",
      " 5   WorkDuration        204277 non-null  int64  \n",
      " 6   ActiveAccounts      204277 non-null  int64  \n",
      " 7   OfferRate           204277 non-null  float64\n",
      " 8   RepayPeriod         204277 non-null  int64  \n",
      " 9   DebtFactor          204277 non-null  float64\n",
      " 10  QualificationLevel  204277 non-null  object \n",
      " 11  WorkCategory        204277 non-null  object \n",
      " 12  RelationshipStatus  204277 non-null  object \n",
      " 13  OwnsProperty        204277 non-null  object \n",
      " 14  FamilyObligation    204277 non-null  object \n",
      " 15  FundUseCase         204277 non-null  object \n",
      " 16  JointApplicant      204277 non-null  object \n",
      " 17  RiskFlag            204277 non-null  int64  \n",
      "dtypes: float64(2), int64(8), object(8)\n",
      "memory usage: 28.1+ MB\n",
      "None\n",
      "\n",
      "Dataset Shape: (204277, 18)\n",
      "\n",
      "Statistical Summary:\n",
      "       ApplicantYears  AnnualEarnings   RequestedSum    TrustMetric  \\\n",
      "count   204277.000000   204277.000000  204277.000000  204277.000000   \n",
      "mean        43.489340    82506.227980  127547.496395     574.075500   \n",
      "std         14.995191    38952.103374   70855.064746     158.877098   \n",
      "min         18.000000    15000.000000    5001.000000     300.000000   \n",
      "25%         31.000000    48878.000000   66059.000000     437.000000   \n",
      "50%         43.000000    82400.000000  127603.000000     574.000000   \n",
      "75%         56.000000   116247.000000  188843.000000     712.000000   \n",
      "max         69.000000   149999.000000  249999.000000     849.000000   \n",
      "\n",
      "        WorkDuration  ActiveAccounts      OfferRate    RepayPeriod  \\\n",
      "count  204277.000000   204277.000000  204277.000000  204277.000000   \n",
      "mean       59.508511        2.502078      13.488147      36.010926   \n",
      "std        34.645589        1.116898       6.636060      16.944827   \n",
      "min         0.000000        1.000000       2.000000      12.000000   \n",
      "25%        30.000000        2.000000       7.760000      24.000000   \n",
      "50%        59.000000        3.000000      13.450000      36.000000   \n",
      "75%        90.000000        4.000000      19.240000      48.000000   \n",
      "max       119.000000        4.000000      25.000000      60.000000   \n",
      "\n",
      "          DebtFactor       RiskFlag  \n",
      "count  204277.000000  204277.000000  \n",
      "mean        0.500579       0.116278  \n",
      "std         0.230914       0.320559  \n",
      "min         0.100000       0.000000  \n",
      "25%         0.300000       0.000000  \n",
      "50%         0.500000       0.000000  \n",
      "75%         0.700000       0.000000  \n",
      "max         0.900000       1.000000  \n",
      "\n",
      "Missing Values:\n",
      "Empty DataFrame\n",
      "Columns: [Missing_Count, Percentage]\n",
      "Index: []\n",
      "\n",
      "Data Types:\n",
      "object     8\n",
      "int64      8\n",
      "float64    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Duplicate Rows: 0\n",
      "\n",
      "Unique Values in Categorical Columns:\n",
      "ProfileID: 204277 unique values\n",
      "QualificationLevel: 4 unique values\n",
      "   -> [Low Cardinality] Values: ['High School', \"Master's\", \"Bachelor's\", 'PhD']\n",
      "WorkCategory: 4 unique values\n",
      "   -> [Low Cardinality] Values: ['Self-employed', 'Unemployed', 'Part-time', 'Full-time']\n",
      "RelationshipStatus: 3 unique values\n",
      "   -> [Low Cardinality] Values: ['Single', 'Divorced', 'Married']\n",
      "OwnsProperty: 2 unique values\n",
      "   -> [Low Cardinality] Values: ['Yes', 'No']\n",
      "FamilyObligation: 2 unique values\n",
      "   -> [Low Cardinality] Values: ['No', 'Yes']\n",
      "FundUseCase: 5 unique values\n",
      "   -> [Low Cardinality] Values: ['Business', 'Education', 'Other', 'Auto', 'Home']\n",
      "JointApplicant: 2 unique values\n",
      "   -> [Low Cardinality] Values: ['No', 'Yes']\n",
      "\n",
      "[SUCCESS] Low cardinality categories saved to 'categorical_config.json'\n",
      "\n",
      "==================================================\n",
      "EXPLORATORY DATA ANALYSIS for test\n",
      "==================================================\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51070 entries, 0 to 51069\n",
      "Data columns (total 17 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   ProfileID           51070 non-null  object \n",
      " 1   ApplicantYears      51070 non-null  float64\n",
      " 2   AnnualEarnings      51070 non-null  float64\n",
      " 3   RequestedSum        51070 non-null  float64\n",
      " 4   TrustMetric         51070 non-null  float64\n",
      " 5   WorkDuration        51070 non-null  float64\n",
      " 6   ActiveAccounts      51070 non-null  float64\n",
      " 7   OfferRate           51070 non-null  float64\n",
      " 8   RepayPeriod         51070 non-null  float64\n",
      " 9   DebtFactor          51070 non-null  float64\n",
      " 10  QualificationLevel  51070 non-null  object \n",
      " 11  WorkCategory        51070 non-null  object \n",
      " 12  RelationshipStatus  51070 non-null  object \n",
      " 13  OwnsProperty        51070 non-null  object \n",
      " 14  FamilyObligation    51070 non-null  object \n",
      " 15  FundUseCase         51070 non-null  object \n",
      " 16  JointApplicant      51070 non-null  object \n",
      "dtypes: float64(9), object(8)\n",
      "memory usage: 6.6+ MB\n",
      "None\n",
      "\n",
      "Dataset Shape: (51070, 17)\n",
      "\n",
      "Statistical Summary:\n",
      "       ApplicantYears  AnnualEarnings   RequestedSum   TrustMetric  \\\n",
      "count    51070.000000    51070.000000   51070.000000  51070.000000   \n",
      "mean        43.534169    82471.611474  127704.340141    575.019718   \n",
      "std         14.970605    39006.993391   70783.797718    159.010208   \n",
      "min         18.000000    15000.000000    5000.000000    300.000000   \n",
      "25%         31.000000    48616.750000   66506.250000    437.000000   \n",
      "50%         43.000000    82686.500000  127330.000000    575.000000   \n",
      "75%         57.000000   116136.250000  189465.750000    713.000000   \n",
      "max         69.000000   149994.000000  249986.000000    849.000000   \n",
      "\n",
      "       WorkDuration  ActiveAccounts     OfferRate   RepayPeriod    DebtFactor  \n",
      "count  51070.000000    51070.000000  51070.000000  51070.000000  51070.000000  \n",
      "mean      59.675837        2.496867     13.511278     36.085765      0.498745  \n",
      "std       34.634536        1.117497      6.638008     17.067022      0.230924  \n",
      "min        0.000000        1.000000      2.000000     12.000000      0.100000  \n",
      "25%       30.000000        1.000000      7.800000     24.000000      0.300000  \n",
      "50%       60.000000        2.000000     13.480000     36.000000      0.500000  \n",
      "75%       90.000000        3.000000     19.280000     48.000000      0.700000  \n",
      "max      119.000000        4.000000     25.000000     60.000000      0.900000  \n",
      "\n",
      "Missing Values:\n",
      "Empty DataFrame\n",
      "Columns: [Missing_Count, Percentage]\n",
      "Index: []\n",
      "\n",
      "Data Types:\n",
      "float64    9\n",
      "object     8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Duplicate Rows: 0\n",
      "\n",
      "Unique Values in Categorical Columns:\n",
      "ProfileID: 51070 unique values\n",
      "QualificationLevel: 4 unique values\n",
      "   -> [Low Cardinality] Values: ['PhD', 'High School', \"Master's\", \"Bachelor's\"]\n",
      "WorkCategory: 4 unique values\n",
      "   -> [Low Cardinality] Values: ['Self-employed', 'Part-time', 'Unemployed', 'Full-time']\n",
      "RelationshipStatus: 3 unique values\n",
      "   -> [Low Cardinality] Values: ['Single', 'Divorced', 'Married']\n",
      "OwnsProperty: 2 unique values\n",
      "   -> [Low Cardinality] Values: ['Yes', 'No']\n",
      "FamilyObligation: 2 unique values\n",
      "   -> [Low Cardinality] Values: ['Yes', 'No']\n",
      "FundUseCase: 5 unique values\n",
      "   -> [Low Cardinality] Values: ['Home', 'Education', 'Business', 'Other', 'Auto']\n",
      "JointApplicant: 2 unique values\n",
      "   -> [Low Cardinality] Values: ['No', 'Yes']\n",
      "\n",
      "[SUCCESS] Low cardinality categories saved to 'categorical_config.json'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPLORATORY DATA ANALYSIS for train\")\n",
    "print(\"=\"*50)\n",
    "d = perform_eda(train_df)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPLORATORY DATA ANALYSIS for test\")\n",
    "print(\"=\"*50)\n",
    "d = perform_eda(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17092aaf",
   "metadata": {},
   "source": [
    "## Things left to do\n",
    "\n",
    "Do the following preprocessing steps based on your requirements.\n",
    "\n",
    "1) Feature Engineering\n",
    "2) Feature Encoding\n",
    "4) Encoding Categorical Variables\n",
    "3) Feature Scaling\n",
    "4) Feature Selection\n",
    "5) Data Validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
